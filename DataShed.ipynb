{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataShed Technical Test\n",
    "\n",
    "## Requirements\n",
    "\n",
    "The application should:\n",
    "\n",
    "Read data from the source file\n",
    "Output:\n",
    "\n",
    "1) the number of records in the source data set\n",
    "\n",
    "2) the number of unique records* in the source data set\n",
    "\n",
    "3) the number of different people** in the source data set\n",
    "\n",
    "4) Write the data that is considered duplicated/related to a CSV file called relateddata.csv\n",
    "\n",
    "5) Consider data that is similar and classify it as a duplicated/related. For example \"Wilyam Premadasta\" and \"William Premadasa\" should be considered the same person.\n",
    "\n",
    "*exact duplicate records should be eliminated from this measure.\n",
    "\n",
    "** the number of records following your de-duplication processing.\n",
    "\n",
    "## Solution - Fergus McLellan\n",
    "My solution is based on Python 3, relying heavily on Pandas to import and clean the data. The \"fuzzywuzzy\" module is used to assist with the identification of records which are related with similar given names and surnames. The requirements give no indication as to how much weight should be given to the date of birth, and there are a number of records which have the same names, but a very different date of birth. For the purposes of this solution, it is assumed that the date of birth is correct, and we are identifying records which have the same date of birth and similar names. There is also no direction given as to whether or not any errors in the sex provided should be captured, as there are a large number of \"v\" entries.\n",
    "\n",
    "Due to the use of Pandas, a Jupyter Notebook of the prototype solution is included here for convenience. However, the full solution can be found in the .py file on Github.\n",
    "\n",
    "## Output files\n",
    "<ul>\n",
    "<li>summary.txt - contains the summary information and figures for the number of records and duplicates outlined in the requirements</li>\n",
    "<li>exact_duplicates.csv - contains the records which have an exact duplicate</li>\n",
    "<li>related.csv - contains the data that is considered related, with the related pair records listed side by side</li>\n",
    "<li>cleaned.csv - contains the final data which has had duplicate and the 2nd occurence of a related record removed</li>\n",
    "<li>invalid_data.csv - contains the records which did not contain data in the expected format. Some entries were identified with an inconsistent date format.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import pandas as pd\n",
    "import pandas_schema\n",
    "from pandas_schema.validation import CustomElementValidation\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "import datetime\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the fuzz_score to capture fewer similarities, and return more strict comparisons\n",
    "# Decrease the fuzz_score to capture more vague similarities\n",
    "fuzz_score = 50\n",
    "\n",
    "# Set input file to be analysed here\n",
    "input_file = 'DataShed_Technical_Test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file and check for errors\n",
    "if not os.path.exists(input_file):\n",
    "    print(\"Unable to find filename provided\")\n",
    "\n",
    "try:\n",
    "    df_people_records = pd.read_csv(input_file)\n",
    "\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"Note: the given .csv was empty.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error parsing .csv file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirement 1) the number of records in the source data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records in source: 11122\n"
     ]
    }
   ],
   "source": [
    "# Capture the number of entries in the dataframe, and assert that there are more than 5 rows\n",
    "no_of_records_in_source = df_people_records.shape[0]\n",
    "print(\"Total number of records in source: \" + str(no_of_records_in_source))\n",
    "assert no_of_records_in_source > 5, \".csv file does not have enough entries to perform useful comparisons\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the column names are as expected\n",
    "expected_column_names = ['given_name', 'surname', 'date_of_birth', 'sex']\n",
    "column_assert_message = \"Input file has different column names from the Github sample\"\n",
    "for expected_column in expected_column_names:\n",
    "    assert expected_column in df_people_records.columns, expected_column + \" not found. \" + column_assert_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>given_name</th>\n",
       "      <th>surname</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mitchell</td>\n",
       "      <td>clausson</td>\n",
       "      <td>21/01/1980</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thomas</td>\n",
       "      <td>skindstad</td>\n",
       "      <td>18/06/1988</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>harry</td>\n",
       "      <td>sodho</td>\n",
       "      <td>27/04/1983</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jessica</td>\n",
       "      <td>haward</td>\n",
       "      <td>27/12/1992</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>maya</td>\n",
       "      <td>kurniawan</td>\n",
       "      <td>13/03/1992</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  given_name    surname date_of_birth sex\n",
       "0   mitchell   clausson    21/01/1980   m\n",
       "1     thomas  skindstad    18/06/1988   m\n",
       "2      harry      sodho    27/04/1983   f\n",
       "3    jessica     haward    27/12/1992   f\n",
       "4       maya  kurniawan    13/03/1992   f"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_people_records.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirement 2) the number of unique records in the source data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique records: 10961\n"
     ]
    }
   ],
   "source": [
    "# Duplicates are captured, and saved to a separate file, exact_duplicates.csv\n",
    "df_duplicate_records = df_people_records[pd.DataFrame.duplicated(df_people_records)]\n",
    "df_duplicate_records.to_csv('exact_duplicates.csv', index=False)\n",
    "\n",
    "# Remove duplicates from current dataFrame\n",
    "df_people_records = df_people_records.drop_duplicates()\n",
    "number_of_unique_records = df_people_records.shape[0]\n",
    "print(\"Number of unique records: \" + str(number_of_unique_records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that all input data contains string or date information \n",
    "# The bulk of the date of birth data is in dd/mm/YYYY date format.\n",
    "# However, a small number of entries were found with inconsistent dates.\n",
    "date_format = \"%d/%m/%Y\"\n",
    "\n",
    "def check_string(a_string):\n",
    "    try:\n",
    "        str(a_string)\n",
    "    except InvalidOperation:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def check_date(a_date):\n",
    "    try:\n",
    "        datetime.datetime.strptime(a_date, date_format)\n",
    "    except ValueError:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "string_validation = [CustomElementValidation(lambda s: check_string(s), 'is not a string')]\n",
    "date_validation = [CustomElementValidation(lambda d: check_date(d), 'is not a correct date')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = pandas_schema.Schema([\n",
    "            pandas_schema.Column('given_name', string_validation),\n",
    "            pandas_schema.Column('surname', string_validation),\n",
    "            pandas_schema.Column('date_of_birth', date_validation),\n",
    "            pandas_schema.Column('sex', string_validation)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply data type validation\n",
    "errors = schema.validate(df_people_records)\n",
    "errors_index_rows = [error.row for error in errors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data in these rows did not match expected data type or format:\n",
      "     given_name    surname date_of_birth sex\n",
      "2193     daniel    rennoll    1987-12-32   m\n",
      "2714     olivia  adolfsson    1995-12-32   f\n",
      "8465       esme  muraguchi    1987-12-32   f\n",
      "8582       kyle    siregar    2009-12-32   f\n",
      "8978     brooke     strapp    1990-12-32   f\n"
     ]
    }
   ],
   "source": [
    "if len(errors_index_rows) > 0:\n",
    "    df_invalid_data = df_people_records[df_people_records.index.isin(errors_index_rows)]\n",
    "    print(\"Data in these rows did not match expected data type or format:\")\n",
    "    print(df_invalid_data)\n",
    "    df_invalid_data.to_csv('invalid_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New dataframe with any invalid data rows dropped\n",
    "df_cleaned_records = df_people_records.drop(index=errors_index_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unicode data found in data source - this makes comparison operations difficult.\n",
    "# Convert unicode characters into their ascii equivalent equivalent characters\n",
    "df_cleaned_records[\"ascii_given_name\"] = df_cleaned_records['given_name'].apply(lambda x: str(unicodedata.normalize('NFKD', str(x)).encode('ascii', 'ignore')).split(\"'\")[1])\n",
    "df_cleaned_records[\"ascii_surname\"] = df_cleaned_records['surname'].apply(lambda x: str(unicodedata.normalize('NFKD', str(x)).encode('ascii', 'ignore')).split(\"'\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove anything which is not a letter, e.g. hyphen, apostrophes, etc.\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "df_cleaned_records[\"ascii_given_name\"] = df_cleaned_records[\"ascii_given_name\"].apply(lambda x: regex.sub('', x))\n",
    "df_cleaned_records[\"ascii_surname\"] = df_cleaned_records[\"ascii_surname\"].apply(lambda x: regex.sub('', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_similar_record(input_date):\n",
    "    \"\"\"\n",
    "    check_for_similar_record - takes a date as input\n",
    "    Retrieves all rows which include that date, and creates dictionaries of the given names and surnames.\n",
    "    Fuzzywuzzy is used to score the similarity between the given names and the surnames.\n",
    "    \n",
    "    Any pair of row indexes which have a score of greater than 'fuzz_score' (set to 50, but this can be modified)\n",
    "    for both the given name and the surname are considered to be similar.\n",
    "    \n",
    "    Output: row indices for the 2 records which are considered to be similar\n",
    "    \"\"\"\n",
    "    given_names_dict = dict(enumerate(df_cleaned_records[df_cleaned_records['date_of_birth'] == input_date]['ascii_given_name']))\n",
    "    surnames_dict = dict(enumerate(df_cleaned_records[df_cleaned_records['date_of_birth'] == input_date]['ascii_surname']))\n",
    "    index_dict = dict(enumerate(df_cleaned_records[df_cleaned_records['date_of_birth'] == input_date].index))\n",
    "    for this_record in range(len(surnames_dict)):\n",
    "        for compare_record in range(len(surnames_dict)):\n",
    "            # do not compare record against itself\n",
    "            if not compare_record == this_record:\n",
    "                given_name_score =  fuzz.ratio(given_names_dict[this_record], given_names_dict[compare_record])\n",
    "                if given_name_score > fuzz_score:\n",
    "                    surname_score =  fuzz.ratio(surnames_dict[this_record], surnames_dict[compare_record])\n",
    "                    if surname_score > fuzz_score:\n",
    "                        return index_dict[this_record], index_dict[compare_record]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of tuples, which contain the dataFrame indices for any matching record pairs\n",
    "fuzzy_matched_records = [check_for_similar_record(x) for x in df_cleaned_records[df_cleaned_records.duplicated(subset=['date_of_birth'])]['date_of_birth'].unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any empty entries from list\n",
    "if len(fuzzy_matched_records) > 0:\n",
    "    fuzzy_matched_records = [idx for idx in fuzzy_matched_records if idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame of the fuzzy matched records \n",
    "if len(fuzzy_matched_records) > 0:\n",
    "    df_fuzzy_matched_records = pd.concat([\n",
    "        pd.DataFrame([df_cleaned_records.loc[idx[0]] for idx in fuzzy_matched_records]).reset_index(drop=True), \n",
    "        pd.DataFrame([df_cleaned_records.loc[idx[1]] for idx in fuzzy_matched_records]).reset_index(drop=True)], \n",
    "        axis=1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Ascii versions of columns to leave the original encoding versions\n",
    "df_fuzzy_matched_records.drop([4, 5, 10, 11], axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename fuzzy matched columns\n",
    "df_fuzzy_matched_records.columns = ['given_name_A', 'surname_A', 'date_of_birth_A', 'sex_A', \n",
    "                                    'given_name_B', 'surname_B', 'date_of_birth_B', 'sex_B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_pairs_of_similar_records = df_fuzzy_matched_records.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 2nd fuzzy match from original dataframe, and drop the ascii columns\n",
    "df_cleaned_records.drop([idx[1] for idx in fuzzy_matched_records if idx], axis=0, inplace=True)\n",
    "df_cleaned_records.drop(columns=['ascii_given_name', 'ascii_surname'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned_records.to_csv('cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirement 3) the number of different people in the source data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different people: 10120\n"
     ]
    }
   ],
   "source": [
    "number_of_different_people = df_cleaned_records.shape[0]\n",
    "print(\"Number of different people: \" + str(number_of_different_people))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirement 4) Write the data that is considered duplicated/related to a CSV file called relateddata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fuzzy_matched_records.to_csv('relateddata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary file\n",
    "output_file = open(\"summary.txt\", \"w\")\n",
    "output_file.write(\"Summary information on the records in the input file: \" + input_file + '\\n')\n",
    "output_file.write(\"====================================================================\" + '\\n\\n')\n",
    "output_file.write(\"Total number of records in source: \" + str(no_of_records_in_source) + '\\n')\n",
    "output_file.write(\"Number of unique records: \" + str(number_of_unique_records) + '\\n')\n",
    "output_file.write(\"Number of pairs of similar records: \" + str(number_of_pairs_of_similar_records) + '\\n')\n",
    "output_file.write(\"Number of different people: \" + str(number_of_different_people) + '\\n')\n",
    "output_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
